[
  {
    "objectID": "aoi_report/aoi_reports/FLA_KEYS_.html",
    "href": "aoi_report/aoi_reports/FLA_KEYS_.html",
    "title": "FLA_KEYS_ Report",
    "section": "",
    "text": "FLA_KEYS_\n\n\nselect only unique coverage points\n# Load necessary libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nselect only unique coverage points\nlibrary(glue)\nlibrary(here)\n\n\nhere() starts at /home/tylar/repos/rvc_percent_cover\n\n\nselect only unique coverage points\npattern &lt;- paste0({params$inputFilePrefix}, \"\\\\d{4}\\\\.csv\")\n\n# List all files matching the pattern\nfiles &lt;- list.files(\n  path = here(\"data/01_raw\"), pattern = pattern, full.names = TRUE\n)\n\n# Initialize an empty list to store data\ndata_list &lt;- list()\n\n# Loop over the files\nfor (file in files) {\n  # Read the CSV file\n  data &lt;- read.csv(file)\n  print(glue(\"reading {file}...\"))\n  \n  # Select the desired columns\n  selected_data &lt;- data %&gt;%\n    select(LAT_DEGREES, LON_DEGREES, YEAR, MONTH, DAY, HABITAT_CD) %&gt;%\n    mutate(date = sprintf(\"%04d-%02d-%02d\", YEAR, MONTH, DAY)) %&gt;%\n    mutate(`system:time_start` = as.numeric(as.POSIXct(\n      date, format=\"%Y-%m-%d\", tz=\"UTC\"\n    )) * 1000) %&gt;%\n    select( -MONTH, -DAY) %&gt;%\n    rename(latitude = LAT_DEGREES, longitude = LON_DEGREES)\n  \n  # Keep only the rows with unique values across the selected columns\n  unique_data &lt;- selected_data %&gt;%\n    distinct()\n  \n  # Append the data to the list\n  data_list[[length(data_list) + 1]] &lt;- unique_data\n}\n\n\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_1999.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2000.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2001.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2002.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2003.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2004.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2005.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2006.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2007.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2008.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2009.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2010.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2011.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2012.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2014.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2016.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2018.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FLA_KEYS_2022.csv...\n\n\nselect only unique coverage points\n# Combine all data into a single dataframe\ncombined_data &lt;- do.call(rbind, data_list)\n\n\n\n\nmap the points\nlibrary(sf)\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n\nmap the points\nlibrary(ggplot2)\nlibrary(ggspatial)\n# Convert the data to a spatial format (sf object)\nselected_data_sf &lt;- st_as_sf(combined_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Plot the points on a map\nggplot(data = selected_data_sf) +\n  annotation_map_tile(type = \"osm\") +  # Add a basemap (OpenStreetMap)\n  geom_sf(aes(color = HABITAT_CD), size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Map of Latitude and Longitude Points with Basemap\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Habitat Code\") +\n  theme(legend.position = \"bottom\")\n\n\nLoading required namespace: raster\n\n\nZoom: 7\n\n\n\n\n\n\n\n\n\n\n\n\nhab code\nhabitat description\n\n\n\n\nCONT_LR\ncontinuous low relief\n\n\nISOL_MR\nisolated mid-relief\n\n\nISOL_LR\nisolated low-relief\n\n\nSPGR_LR\nspur and groove low relief\n\n\nSPGR_HR\nspur and groove high relief\n\n\nRUBB_LR\nrubble low relief\n\n\nSAND_NA\nSand\n\n\nSGRS_NA\nSeagrass\n\n\nUCHB_LR\nUnconsolidated Hardbottom\n\n\n\n\n\nsee distribution of cover points\nlibrary(ggplot2)\n\nggplot(combined_data, aes(x = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Bar Chart of HABITAT_CD\", x = \"Habitat Code\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nsee distribution of points over time\nlibrary(ggplot2)\n\n# Create the stacked bar chart\nggplot(combined_data, aes(x = YEAR, fill = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Points Collected Over Time\", x = \"Year\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nfinal data cleaning & write to csv\n# Display the filtered unique data\ncombined_data &lt;- combined_data %&gt;%\n  select(-YEAR)\n\nwrite.csv(combined_data, here(glue(\"data/02_reduced/{params$inputFile}.csv\")), row.names=FALSE)"
  },
  {
    "objectID": "aoi_report/aoi_reports/DRY_TORT_.html",
    "href": "aoi_report/aoi_reports/DRY_TORT_.html",
    "title": "DRY_TORT_ Report",
    "section": "",
    "text": "DRY_TORT_\n\n\nselect only unique coverage points\n# Load necessary libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nselect only unique coverage points\nlibrary(glue)\nlibrary(here)\n\n\nhere() starts at /home/tylar/repos/rvc_percent_cover\n\n\nselect only unique coverage points\npattern &lt;- paste0({params$inputFilePrefix}, \"\\\\d{4}\\\\.csv\")\n\n# List all files matching the pattern\nfiles &lt;- list.files(\n  path = here(\"data/01_raw\"), pattern = pattern, full.names = TRUE\n)\n\n# Initialize an empty list to store data\ndata_list &lt;- list()\n\n# Loop over the files\nfor (file in files) {\n  # Read the CSV file\n  data &lt;- read.csv(file)\n  print(glue(\"reading {file}...\"))\n  \n  # Select the desired columns\n  selected_data &lt;- data %&gt;%\n    select(LAT_DEGREES, LON_DEGREES, YEAR, MONTH, DAY, HABITAT_CD) %&gt;%\n    mutate(date = sprintf(\"%04d-%02d-%02d\", YEAR, MONTH, DAY)) %&gt;%\n    mutate(`system:time_start` = as.numeric(as.POSIXct(\n      date, format=\"%Y-%m-%d\", tz=\"UTC\"\n    )) * 1000) %&gt;%\n    select( -MONTH, -DAY) %&gt;%\n    rename(latitude = LAT_DEGREES, longitude = LON_DEGREES)\n  \n  # Keep only the rows with unique values across the selected columns\n  unique_data &lt;- selected_data %&gt;%\n    distinct()\n  \n  # Append the data to the list\n  data_list[[length(data_list) + 1]] &lt;- unique_data\n}\n\n\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_1999.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2000.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2004.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2006.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2008.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2010.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2012.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2014.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2016.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2018.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2021.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2023.csv...\n\n\nselect only unique coverage points\n# Combine all data into a single dataframe\ncombined_data &lt;- do.call(rbind, data_list)\n\n\n\n\nmap the points\nlibrary(sf)\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n\nmap the points\nlibrary(ggplot2)\nlibrary(ggspatial)\n# Convert the data to a spatial format (sf object)\nselected_data_sf &lt;- st_as_sf(combined_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Plot the points on a map\nggplot(data = selected_data_sf) +\n  annotation_map_tile(type = \"osm\") +  # Add a basemap (OpenStreetMap)\n  geom_sf(aes(color = HABITAT_CD), size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Map of Latitude and Longitude Points with Basemap\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Habitat Code\") +\n  theme(legend.position = \"bottom\")\n\n\nLoading required namespace: raster\n\n\nZoom: 10\n\n\n\n\n\n\n\n\n\n\n\n\nhab code\nhabitat description\n\n\n\n\nCONT_LR\ncontinuous low relief\n\n\nISOL_MR\nisolated mid-relief\n\n\nISOL_LR\nisolated low-relief\n\n\nSPGR_LR\nspur and groove low relief\n\n\nSPGR_HR\nspur and groove high relief\n\n\nRUBB_LR\nrubble low relief\n\n\nSAND_NA\nSand\n\n\nSGRS_NA\nSeagrass\n\n\nUCHB_LR\nUnconsolidated Hardbottom\n\n\n\n\n\nsee distribution of cover points\nlibrary(ggplot2)\n\nggplot(combined_data, aes(x = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Bar Chart of HABITAT_CD\", x = \"Habitat Code\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nsee distribution of points over time\nlibrary(ggplot2)\n\n# Create the stacked bar chart\nggplot(combined_data, aes(x = YEAR, fill = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Points Collected Over Time\", x = \"Year\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nfinal data cleaning & write to csv\n# Display the filtered unique data\ncombined_data &lt;- combined_data %&gt;%\n  select(-YEAR)\n\nwrite.csv(combined_data, here(glue(\"data/02_reduced/{params$inputFile}.csv\")), row.names=FALSE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "aoi_report/aoi_reports.html",
    "href": "aoi_report/aoi_reports.html",
    "title": "Area Reports",
    "section": "",
    "text": "DRY_TORT_ Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFGBNMS_ Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFLA_KEYS_ Report\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rvc_percent_cover",
    "section": "",
    "text": "This project tranforms Reef Visual Census data for the Florida Keys for use in assessment of bottom cover maps.\n.csv files produced from this code can be uploaded into Google Earth Engine (GEE) as a table asset. Tables can then be used as a FeatureCollection, filtered, and displayed:\nvar table = ee.FeatureCollection(\"projects/imars-ee/assets/RVC/FLA_KEYS\")\n  .filterDate(\"2020-01-01\", \"2024-01-01\")\n  .filter(ee.Filter.eq(\"HABITAT_CD\", \"CONT_LR\"))\n;\n  \nMap.addLayer(fc)\nMap.centerObject(fc)\nLinks for direct access to tables already uploaded into GEE:\n\nDRY_TORT\nFLA_KEYS\nFGBNMS\n\nSource code for this site and analysis can be found here."
  },
  {
    "objectID": "aoi_report/aoi_report_template.html",
    "href": "aoi_report/aoi_report_template.html",
    "title": "DRY_TORT_ Report",
    "section": "",
    "text": "DRY_TORT_\n\n\nselect only unique coverage points\n# Load necessary libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nselect only unique coverage points\nlibrary(glue)\nlibrary(here)\n\n\nhere() starts at /home/tylar/repos/rvc_percent_cover\n\n\nselect only unique coverage points\npattern &lt;- paste0({params$inputFilePrefix}, \"\\\\d{4}\\\\.csv\")\n\n# List all files matching the pattern\nfiles &lt;- list.files(\n  path = here(\"data/01_raw\"), pattern = pattern, full.names = TRUE\n)\n\n# Initialize an empty list to store data\ndata_list &lt;- list()\n\n# Loop over the files\nfor (file in files) {\n  # Read the CSV file\n  data &lt;- read.csv(file)\n  print(glue(\"reading {file}...\"))\n  \n  # Select the desired columns\n  selected_data &lt;- data %&gt;%\n    select(LAT_DEGREES, LON_DEGREES, YEAR, MONTH, DAY, HABITAT_CD) %&gt;%\n    mutate(date = sprintf(\"%04d-%02d-%02d\", YEAR, MONTH, DAY)) %&gt;%\n    mutate(`system:time_start` = as.numeric(as.POSIXct(\n      date, format=\"%Y-%m-%d\", tz=\"UTC\"\n    )) * 1000) %&gt;%\n    select( -MONTH, -DAY) %&gt;%\n    rename(latitude = LAT_DEGREES, longitude = LON_DEGREES)\n  \n  # Keep only the rows with unique values across the selected columns\n  unique_data &lt;- selected_data %&gt;%\n    distinct()\n  \n  # Append the data to the list\n  data_list[[length(data_list) + 1]] &lt;- unique_data\n}\n\n\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_1999.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2000.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2004.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2006.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2008.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2010.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2012.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2014.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2016.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2018.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2021.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/DRY_TORT_2023.csv...\n\n\nselect only unique coverage points\n# Combine all data into a single dataframe\ncombined_data &lt;- do.call(rbind, data_list)\n\n\n\n\nmap the points\nlibrary(sf)\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n\nmap the points\nlibrary(ggplot2)\nlibrary(ggspatial)\n# Convert the data to a spatial format (sf object)\nselected_data_sf &lt;- st_as_sf(combined_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Plot the points on a map\nggplot(data = selected_data_sf) +\n  annotation_map_tile(type = \"osm\") +  # Add a basemap (OpenStreetMap)\n  geom_sf(aes(color = HABITAT_CD), size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Map of Latitude and Longitude Points with Basemap\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Habitat Code\") +\n  theme(legend.position = \"bottom\")\n\n\nLoading required namespace: raster\n\n\nZoom: 10\n\n\n\n\n\n\n\n\n\n\n\n\nhab code\nhabitat description\n\n\n\n\nCONT_LR\ncontinuous low relief\n\n\nISOL_MR\nisolated mid-relief\n\n\nISOL_LR\nisolated low-relief\n\n\nSPGR_LR\nspur and groove low relief\n\n\nSPGR_HR\nspur and groove high relief\n\n\nRUBB_LR\nrubble low relief\n\n\nSAND_NA\nSand\n\n\nSGRS_NA\nSeagrass\n\n\nUCHB_LR\nUnconsolidated Hardbottom\n\n\n\n\n\nsee distribution of cover points\nlibrary(ggplot2)\n\nggplot(combined_data, aes(x = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Bar Chart of HABITAT_CD\", x = \"Habitat Code\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nsee distribution of points over time\nlibrary(ggplot2)\n\n# Create the stacked bar chart\nggplot(combined_data, aes(x = YEAR, fill = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Points Collected Over Time\", x = \"Year\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nfinal data cleaning & write to csv\n# Display the filtered unique data\ncombined_data &lt;- combined_data %&gt;%\n  select(-YEAR)\n\nwrite.csv(combined_data, here(glue(\"data/02_reduced/{params$inputFile}.csv\")), row.names=FALSE)"
  },
  {
    "objectID": "aoi_report/aoi_reports/FGBNMS_.html",
    "href": "aoi_report/aoi_reports/FGBNMS_.html",
    "title": "FGBNMS_ Report",
    "section": "",
    "text": "FGBNMS_\n\n\nselect only unique coverage points\n# Load necessary libraries\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nselect only unique coverage points\nlibrary(glue)\nlibrary(here)\n\n\nhere() starts at /home/tylar/repos/rvc_percent_cover\n\n\nselect only unique coverage points\npattern &lt;- paste0({params$inputFilePrefix}, \"\\\\d{4}\\\\.csv\")\n\n# List all files matching the pattern\nfiles &lt;- list.files(\n  path = here(\"data/01_raw\"), pattern = pattern, full.names = TRUE\n)\n\n# Initialize an empty list to store data\ndata_list &lt;- list()\n\n# Loop over the files\nfor (file in files) {\n  # Read the CSV file\n  data &lt;- read.csv(file)\n  print(glue(\"reading {file}...\"))\n  \n  # Select the desired columns\n  selected_data &lt;- data %&gt;%\n    select(LAT_DEGREES, LON_DEGREES, YEAR, MONTH, DAY, HABITAT_CD) %&gt;%\n    mutate(date = sprintf(\"%04d-%02d-%02d\", YEAR, MONTH, DAY)) %&gt;%\n    mutate(`system:time_start` = as.numeric(as.POSIXct(\n      date, format=\"%Y-%m-%d\", tz=\"UTC\"\n    )) * 1000) %&gt;%\n    select( -MONTH, -DAY) %&gt;%\n    rename(latitude = LAT_DEGREES, longitude = LON_DEGREES)\n  \n  # Keep only the rows with unique values across the selected columns\n  unique_data &lt;- selected_data %&gt;%\n    distinct()\n  \n  # Append the data to the list\n  data_list[[length(data_list) + 1]] &lt;- unique_data\n}\n\n\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FGBNMS_2018.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FGBNMS_2022.csv...\nreading /home/tylar/repos/rvc_percent_cover/data/01_raw/FGBNMS_2023.csv...\n\n\nselect only unique coverage points\n# Combine all data into a single dataframe\ncombined_data &lt;- do.call(rbind, data_list)\n\n\n\n\nmap the points\nlibrary(sf)\n\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\n\n\nmap the points\nlibrary(ggplot2)\nlibrary(ggspatial)\n# Convert the data to a spatial format (sf object)\nselected_data_sf &lt;- st_as_sf(combined_data, coords = c(\"longitude\", \"latitude\"), crs = 4326)\n\n# Plot the points on a map\nggplot(data = selected_data_sf) +\n  annotation_map_tile(type = \"osm\") +  # Add a basemap (OpenStreetMap)\n  geom_sf(aes(color = HABITAT_CD), size = 2, alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"Map of Latitude and Longitude Points with Basemap\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Habitat Code\") +\n  theme(legend.position = \"bottom\")\n\n\nLoading required namespace: raster\n\n\nZoom: 11\n\n\n\n\n\n\n\n\n\n\n\n\nhab code\nhabitat description\n\n\n\n\nCONT_LR\ncontinuous low relief\n\n\nISOL_MR\nisolated mid-relief\n\n\nISOL_LR\nisolated low-relief\n\n\nSPGR_LR\nspur and groove low relief\n\n\nSPGR_HR\nspur and groove high relief\n\n\nRUBB_LR\nrubble low relief\n\n\nSAND_NA\nSand\n\n\nSGRS_NA\nSeagrass\n\n\nUCHB_LR\nUnconsolidated Hardbottom\n\n\n\n\n\nsee distribution of cover points\nlibrary(ggplot2)\n\nggplot(combined_data, aes(x = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Bar Chart of HABITAT_CD\", x = \"Habitat Code\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nsee distribution of points over time\nlibrary(ggplot2)\n\n# Create the stacked bar chart\nggplot(combined_data, aes(x = YEAR, fill = HABITAT_CD)) +\n  geom_bar() +\n  labs(title = \"Points Collected Over Time\", x = \"Year\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nfinal data cleaning & write to csv\n# Display the filtered unique data\ncombined_data &lt;- combined_data %&gt;%\n  select(-YEAR)\n\nwrite.csv(combined_data, here(glue(\"data/02_reduced/{params$inputFile}.csv\")), row.names=FALSE)"
  }
]